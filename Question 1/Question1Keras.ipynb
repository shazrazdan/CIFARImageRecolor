{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models for Text using LSTMs\n",
    "### Ques (a)\n",
    "In this problem, we are trying to build a generative model to mimic the writ- ing style of prominent British Mathematician, Philosopher, prolific writer, and political activist, Bertrand Russell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9MpjSzf6W15D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "import time\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Cc6DZOxzW15I"
   },
   "outputs": [],
   "source": [
    "window_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques (b)\n",
    "Download the following books from Project Gutenberg http://www.gutenberg.org/ebooks/author/355 in text format:\n",
    "<ol>\n",
    "<li> The Problems of Philosophy\n",
    "<li> The Analysis of Mind\n",
    "<li> Mysticism and Logic and Other Essays\n",
    "<li> Our Knowledge of the External World as a Field for Scientific Method in Philosophy\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques (c) i.\n",
    "Concatenate your text files to create a corpus of Russell’s writings.\n",
    "<br>These books are stripped of their headers and then concatenated in a single corpus.txt which is used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1524529003397,
     "user": {
      "displayName": "Shashwat Razdan",
      "photoUrl": "//lh3.googleusercontent.com/-uysMKqw2z1w/AAAAAAAAAAI/AAAAAAAApxk/W-ioq74c-Qg/s50-c-k-no/photo.jpg",
      "userId": "111186949356327872675"
     },
     "user_tz": 420
    },
    "id": "wD2eofFFW15K",
    "outputId": "8a400f72-b605-4656-ae31-69bc9f833713"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'number of chars = 400000'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"\"\n",
    "linecount = 0\n",
    "with open(\"corpus.txt\", encoding=\"utf-8\") as fileobj:\n",
    "    for line in fileobj: \n",
    "        inp += \" \" + line[:-1]\n",
    "inp = inp[:400000]\n",
    "\"number of chars = \" + str(len(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have taken only 400,000 characters because my kernel would always crash on taking more characters. It would throw out of memory error. \n",
    "### Ques (c) ii.\n",
    "Use a character-level representation for this model by using extended ASCII that has N = 256 characters. Each character will be encoded into a an integer using its ASCII code. Rescale the integers to the range [0, 1], because LSTM uses a sigmoid activation function. LSTM will receive the rescaled integers\n",
    "as its input.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1524529006084,
     "user": {
      "displayName": "Shashwat Razdan",
      "photoUrl": "//lh3.googleusercontent.com/-uysMKqw2z1w/AAAAAAAAAAI/AAAAAAAApxk/W-ioq74c-Qg/s50-c-k-no/photo.jpg",
      "userId": "111186949356327872675"
     },
     "user_tz": 420
    },
    "id": "48m6RNVDW15Q",
    "outputId": "90a72f92-ea13-4665-c4f7-b46f29030907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chars = 87\n",
      "[' ', '!', '\"', '&', \"'\", '(', ')', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'é', 'î', 'ï', 'ö', 'ü', 'œ']\n",
      "\n",
      "{' ': 0, '!': 1, '\"': 2, '&': 3, \"'\": 4, '(': 5, ')': 6, '+': 7, ',': 8, '-': 9, '.': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '=': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'E': 29, 'F': 30, 'G': 31, 'H': 32, 'I': 33, 'J': 34, 'K': 35, 'L': 36, 'M': 37, 'N': 38, 'O': 39, 'P': 40, 'Q': 41, 'R': 42, 'S': 43, 'T': 44, 'U': 45, 'V': 46, 'W': 47, 'X': 48, 'Y': 49, 'Z': 50, '[': 51, ']': 52, '_': 53, 'a': 54, 'b': 55, 'c': 56, 'd': 57, 'e': 58, 'f': 59, 'g': 60, 'h': 61, 'i': 62, 'j': 63, 'k': 64, 'l': 65, 'm': 66, 'n': 67, 'o': 68, 'p': 69, 'q': 70, 'r': 71, 's': 72, 't': 73, 'u': 74, 'v': 75, 'w': 76, 'x': 77, 'y': 78, 'z': 79, 'æ': 80, 'é': 81, 'î': 82, 'ï': 83, 'ö': 84, 'ü': 85, 'œ': 86}\n",
      "\n",
      "{0: ' ', 1: '!', 2: '\"', 3: '&', 4: \"'\", 5: '(', 6: ')', 7: '+', 8: ',', 9: '-', 10: '.', 11: '0', 12: '1', 13: '2', 14: '3', 15: '4', 16: '5', 17: '6', 18: '7', 19: '8', 20: '9', 21: ':', 22: ';', 23: '=', 24: '?', 25: 'A', 26: 'B', 27: 'C', 28: 'D', 29: 'E', 30: 'F', 31: 'G', 32: 'H', 33: 'I', 34: 'J', 35: 'K', 36: 'L', 37: 'M', 38: 'N', 39: 'O', 40: 'P', 41: 'Q', 42: 'R', 43: 'S', 44: 'T', 45: 'U', 46: 'V', 47: 'W', 48: 'X', 49: 'Y', 50: 'Z', 51: '[', 52: ']', 53: '_', 54: 'a', 55: 'b', 56: 'c', 57: 'd', 58: 'e', 59: 'f', 60: 'g', 61: 'h', 62: 'i', 63: 'j', 64: 'k', 65: 'l', 66: 'm', 67: 'n', 68: 'o', 69: 'p', 70: 'q', 71: 'r', 72: 's', 73: 't', 74: 'u', 75: 'v', 76: 'w', 77: 'x', 78: 'y', 79: 'z', 80: 'æ', 81: 'é', 82: 'î', 83: 'ï', 84: 'ö', 85: 'ü', 86: 'œ'}\n"
     ]
    }
   ],
   "source": [
    "unique_chars = list(set(inp))\n",
    "unique_idx = []\n",
    "for char in unique_chars:\n",
    "    unique_idx.append(ord(char))\n",
    "\n",
    "unique_idx = sorted(unique_idx)\n",
    "\n",
    "unique_chars = []\n",
    "for idx in unique_idx:\n",
    "    unique_chars.append(chr(idx))\n",
    "\n",
    "print(\"Number of unique chars =\", len(unique_chars))\n",
    "print(unique_chars)\n",
    "char_to_ix = {}\n",
    "ix_to_char = {}\n",
    "print()\n",
    "\n",
    "for index, char in enumerate(unique_chars):\n",
    "    char_to_ix[char] = index\n",
    "    ix_to_char[index] = char\n",
    "    \n",
    "print(char_to_ix)\n",
    "print()\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 87 unique characters, ordered according to their extended ASCII codes and then rescaled from 0-1. The rescaled input is shown in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1524529009229,
     "user": {
      "displayName": "Shashwat Razdan",
      "photoUrl": "//lh3.googleusercontent.com/-uysMKqw2z1w/AAAAAAAAAAI/AAAAAAAApxk/W-ioq74c-Qg/s50-c-k-no/photo.jpg",
      "userId": "111186949356327872675"
     },
     "user_tz": 420
    },
    "id": "MrPRkjHkW15U",
    "outputId": "2ecbf733-9018-4aab-9590-30ac590e3a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.45977011 0.81609195 ... 0.68965517 0.09195402 0.        ]\n"
     ]
    }
   ],
   "source": [
    "lstm_input = []\n",
    "for char in inp:\n",
    "    lstm_input.append(char_to_ix[char])\n",
    "lstm_input = np.array(lstm_input)\n",
    "lstm_input_scaled = lstm_input / len(unique_chars)\n",
    "print(lstm_input_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original ASCII sequence of all the characters is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1524529012207,
     "user": {
      "displayName": "Shashwat Razdan",
      "photoUrl": "//lh3.googleusercontent.com/-uysMKqw2z1w/AAAAAAAAAAI/AAAAAAAApxk/W-ioq74c-Qg/s50-c-k-no/photo.jpg",
      "userId": "111186949356327872675"
     },
     "user_tz": 420
    },
    "id": "lDk8wNRyW15Y",
    "outputId": "efb57b83-d01a-4625-f00e-cb3069db805d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 40 71 ... 60  8  0]\n"
     ]
    }
   ],
   "source": [
    "print(lstm_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques (c) iii.\n",
    "Choose a window size, e.g., W = 100.<br>\n",
    "This is already chosen above and used in parts above as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques (c) iv.\n",
    "Inputs to the network will be the first W −1 = 99 characters of each sequence, and the output of the network will be the Lth character of the sequence. Basically, we are training the network to predict the each character using the 99 characters that precede it. Slide the window in strides of S = 1 on the text. For example, if W = 5 and S = 1 and we want to train the network with the sequence ABRACADABRA, The first input to the network will be ABRA and the corresponding output will be C. The second input will be BRAC and the second output will be A, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t0N3InzFW15b"
   },
   "outputs": [],
   "source": [
    "def get_x_y(inp, lstm_input_scaled, window_size, char_to_ix, ix_to_char, unique_chars):\n",
    "    lstm_inp_stacked = []\n",
    "    lstm_inp_y = []\n",
    "    lstm_input_arr = []\n",
    "    zeros = np.zeros(len(unique_chars))\n",
    "    for i in lstm_input_scaled:\n",
    "        lstm_input_arr.append([i])\n",
    "    for i in range(len(lstm_input)-window_size):\n",
    "        lstm_inp_stacked.append(lstm_input_arr[i:i+window_size-1])\n",
    "        \n",
    "        y = inp[i+window_size-1]\n",
    "        y_ix = char_to_ix[y]\n",
    "        zeros[y_ix] = 1\n",
    "        lstm_inp_y.append(zeros.copy())\n",
    "        zeros[y_ix] = 0\n",
    "    return np.array(lstm_inp_stacked), np.array(lstm_inp_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques (c) v.\n",
    "Note that the output has to be encoded using a one-hot encoding scheme with N = 256 (or less) elements. This means that the network reads integers, but outputs a vector of N = 256 (or less) elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input X and Y are shown below. \n",
    "We have stacked X of width 99 (window_size-1) and the output is one hot encoded representation of output characters. \n",
    "Their shapes are printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16593,
     "status": "ok",
     "timestamp": 1524519857480,
     "user": {
      "displayName": "Shashwat Razdan",
      "photoUrl": "//lh3.googleusercontent.com/-uysMKqw2z1w/AAAAAAAAAAI/AAAAAAAApxk/W-ioq74c-Qg/s50-c-k-no/photo.jpg",
      "userId": "111186949356327872675"
     },
     "user_tz": 420
    },
    "id": "9pSH016EW15e",
    "outputId": "0dee238b-344d-45e7-d7dd-48ef6bc9336e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((399900, 99, 1), (399900, 87))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_inp_stacked, lstm_inp_y = get_x_y(inp, lstm_input_scaled, window_size, char_to_ix, ix_to_char, unique_chars)\n",
    "lstm_inp_stacked.shape, lstm_inp_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that maximum occuring characters in the corpus, I printed its index and indeed, the most occuring character is at index 0 which is a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 537,
     "status": "ok",
     "timestamp": 1524529018324,
     "user": {
      "displayName": "Shashwat Razdan",
      "photoUrl": "//lh3.googleusercontent.com/-uysMKqw2z1w/AAAAAAAAAAI/AAAAAAAApxk/W-ioq74c-Qg/s50-c-k-no/photo.jpg",
      "userId": "111186949356327872675"
     },
     "user_tz": 420
    },
    "id": "KWIvepWSW15h",
    "outputId": "8d645ae2-c820-48ba-e37b-1a188e9bb192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0]),)\n"
     ]
    }
   ],
   "source": [
    "print(np.where(lstm_inp_y[0] == max(lstm_inp_y[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques (c) vi.\n",
    "Use a single hidden layer for the LSTM with N = 256 (or less) memory units.\n",
    "### Ques (c) vii.\n",
    "Use a Softmax output layer to yield a probability prediction for each of the characters between 0 and 1. This is actually a character classification problem with N classes. Choose log loss (cross entropy) as the objective function for the network (research what it menas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3p-6hzEtW15m"
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape, unique_chars):\n",
    "    regressor = Sequential()\n",
    "    regressor.add(LSTM(units = len(unique_chars), return_sequences = False, input_shape = (input_shape, 1)))\n",
    "    \n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    regressor.add(Dense(units = len(unique_chars), activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=False)\n",
    "    \n",
    "    regressor.compile(optimizer = optimizer, loss = 'categorical_crossentropy')\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques (c) viii.\n",
    "We do not use a test dataset. We are using the whole training dataset to learn the probability of each character in a sequence. We are not seeking for a very accurate model of. Instead we are interested in a generalization of the dataset that can mimic the gist of the text.\n",
    "### Ques (c) ix.\n",
    "Choose a reasonable number of epochs for training (e.g., 30, although the network will need more epochs to yield a better model).\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "The number of epochs chosen are 200. Less number of epochs were actually producing garbled results and mostly whitespaces.\n",
    "### Ques (c) x.\n",
    "Use model checkpointing to keep the network weights to determine each time an improvement in loss is observed at the end of the epoch. Find the best set of weights in terms of loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KCvGWif2W15p"
   },
   "outputs": [],
   "source": [
    "filepath=\"weights.best.4lac.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1524529661819,
     "user": {
      "displayName": "Shashwat Razdan",
      "photoUrl": "//lh3.googleusercontent.com/-uysMKqw2z1w/AAAAAAAAAAI/AAAAAAAApxk/W-ioq74c-Qg/s50-c-k-no/photo.jpg",
      "userId": "111186949356327872675"
     },
     "user_tz": 420
    },
    "id": "ksjrBOJLW15t",
    "outputId": "7317dd53-4af3-47da-898d-c7688d8fb6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7ff73875f1d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = create_model(input_shape = window_size-1, unique_chars = unique_chars)\n",
    "regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 147005,
     "status": "ok",
     "timestamp": 1524536489668,
     "user": {
      "displayName": "Shashwat Razdan",
      "photoUrl": "//lh3.googleusercontent.com/-uysMKqw2z1w/AAAAAAAAAAI/AAAAAAAApxk/W-ioq74c-Qg/s50-c-k-no/photo.jpg",
      "userId": "111186949356327872675"
     },
     "user_tz": 420
    },
    "id": "-rWr9uEkC3Hm",
    "outputId": "4f671a41-4538-4a5d-a805-b7337dc36b4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 267933 samples, validate on 131967 samples\n",
      "Epoch 1/1\n",
      "267933/267933 [==============================] - 158s 591us/step - loss: 3.1044 - val_loss: 2.9134\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.91340, saving model to weights.best.4lac.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff738288a58>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(lstm_inp_stacked, lstm_inp_y, validation_split=0.33, epochs=200, shuffle=True, batch_size=1000, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Please note that the training was done on Google Colab which provides a GPU. Final prediction and formatting of the notebook was done on my local machine. Hence, I do not have the training epochs printed below the cell above.<br>\n",
    "The best model was loaded in weights.best.4lac.hdf5 file which was reloaded into the regressor in my local machine.<br>\n",
    "To prove that, I will print final evaluation of the model after 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399900/399900 [==============================] - 648s 2ms/step\n",
      "Final loss after 200 epochs : 2.3951048636418575\n"
     ]
    }
   ],
   "source": [
    "regressor.load_weights(\"weights.best.4lac.hdf5\")\n",
    "print(\"Final loss after 200 epochs :\", regressor.evaluate(lstm_inp_stacked, lstm_inp_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to evaluate model, I predict 300 characters from the training data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1524536971295,
     "user": {
      "displayName": "Shashwat Razdan",
      "photoUrl": "//lh3.googleusercontent.com/-uysMKqw2z1w/AAAAAAAAAAI/AAAAAAAApxk/W-ioq74c-Qg/s50-c-k-no/photo.jpg",
      "userId": "111186949356327872675"
     },
     "user_tz": 420
    },
    "id": "fRS5C6eYW16B",
    "outputId": "fd54ee39-170f-4394-ec75-b86e5828f739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ahe paneee  n tf erto save an  e e  tf  edgtn the pane th the e tf   n   tf thesosophystn thesei th thi h w she     tn th   n e oh theete e   n  tf  e    tn  tne e         ah se tf e   tf er    tn     t  oh te  tf  to the rn  e  the  thete   the    of tetw edge of eot   tncene ceth t  oh   to e    \n",
      "(300, 87)\n"
     ]
    }
   ],
   "source": [
    "pred = regressor.predict(np.array(lstm_inp_stacked[0:300]))\n",
    "print(\"PREDICTION\")\n",
    "for vec in pred:\n",
    "    print(ix_to_char[np.where(vec == max(vec))[0][0]], end=\"\")\n",
    "print(\"\\nACTUAL\")\n",
    "for vec in lstm_inp_y[:300]:\n",
    "    print(ix_to_char[np.where(vec == max(vec))[0][0]], end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques (c) xi.\n",
    "Use the network with the best weights to generate 1000 characters, using the following text as initialization of the network:\n",
    "<br><br>\n",
    "<i>There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "b5QNoXoonEXf"
   },
   "outputs": [],
   "source": [
    "def get_next_char(sequence, regressor, char_to_ix, ix_to_char, unique_chars, window_size):\n",
    "    lstm_input_ = []\n",
    "    for char in sequence:\n",
    "        lstm_input_.append(char_to_ix[char])\n",
    "    lstm_input_ = np.array(lstm_input_)\n",
    "    lstm_input_scaled_ = lstm_input_ / len(unique_chars)\n",
    "    lstm_input_scaled_ = [np.array([lstm_input_scaled_[-99:]]).T]\n",
    "    pred = regressor.predict(np.array(lstm_input_scaled_))\n",
    "    return_seq = \"\"\n",
    "    for vec in pred:\n",
    "        return_seq += ix_to_char[np.where(vec == max(vec))[0][0]]\n",
    "    return return_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphas the seese as the sore the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an the soene an th\n"
     ]
    }
   ],
   "source": [
    "test_sequence = \"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphas\"\n",
    "num_test_pred = 1000\n",
    "result_str = \"\"\n",
    "for i in range(num_test_pred):\n",
    "    result_str += get_next_char(test_sequence + result_str, regressor, char_to_ix, ix_to_char, unique_chars, window_size)\n",
    "print(test_sequence + result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the prediction is a repetitive phrase."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Homework1Keras.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
